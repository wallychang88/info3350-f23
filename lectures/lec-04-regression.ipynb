{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO 3350/6350\n",
    "\n",
    "## Lecture 04: Regression\n",
    "\n",
    "## To do\n",
    "\n",
    "* Waitlist should now be cleared ...\n",
    "* Readings for today/Friday\n",
    "  * NetIDs H-P response to Davis due tonight by 4pm\n",
    "* PS1 due *next* week, not this week\n",
    "  * It will take 5-10 hours. Do not begin working on it the day it's due.\n",
    "* Questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TF-IDF weighting for word counts\n",
    "\n",
    "* Why do we sometimes remove stopwords from our features?\n",
    "    * High-frequency words shared by many documents don't tell us (in many, but not all, cases) much about the similarities or differences between documents\n",
    "* But stopword lists are binary: a word is either a stopword (hence, removed) or it isn't\n",
    "* Can we define a continuous adjustment for \"stoppiness\" that we apply to *every* word, depending on how widely used it is?\n",
    "* One approach is \"term frequency-inverse document frequency\" (TFIDF) weighting. \n",
    "    * You can think of this as multiplying the count of each term in a document by the inverse of the fraction of all documents in which that word occurs (hence \"term frequency [multiplied by] inverse document frequency\"). It's a bit more complicated than that (see below), but that's the idea. This upweights words that occur in relatively few documents.\n",
    "    * The count of a word that occurs in every document would be multiplied by one, hence get no boost in each document. A word that occurs in just one document in a corpus of 100 documents would be multiplied by 100 in the one document that contains it.\n",
    "* There are several tweaks to TFIDF to smooth it out and to modulate the boost it provides. `scikit-learn`'s `TfidfVectorizer` applies the reweighting:\n",
    "\n",
    "$$\\text{idf}(t) = \\ln\\frac{1+n}{1 + \\text{df}(t)} + 1$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $t$ is the term in question\n",
    "* $\\text{idf}(t)$ is the inverse document weight to be applied to the count of term $t$\n",
    "* $n$ is the number of documents in the corpus\n",
    "* $\\text{df}(t)$ is the number of documents in the corpus that contain term $t$\n",
    "\n",
    "A toy example: Consider two documents:\n",
    "\n",
    "* Document 1: `\"cat dog\"`\n",
    "* Document 2: `\"dog dog\"`\n",
    "\n",
    "`cat` occurs in just one document; `dog` occurs in both documents. So we want (and expect) to upweight the count of `cat` in document 1.\n",
    "\n",
    "Calculate the `idf` weight for `cat` in document 1:\n",
    "\n",
    "* $n = 2$\n",
    "* $\\text{df}(\\text{'cat'}) = 1$\n",
    "\n",
    "$$\\text{idf}(\\text{'cat'}) = \\ln\\frac{1 + 2}{1 + 1} + 1 = \\ln\\frac{3}{2} + 1 = 1.405$$\n",
    "\n",
    "And for `dog` in document 1:\n",
    "\n",
    "* $n = 2$\n",
    "* $\\text{df}(\\text{'dog'}) = 2$\n",
    "\n",
    "$$\\text{idf}(\\text{'dog'}) = \\ln\\frac{1 + 2}{1 + 2} + 1 = \\ln\\frac{3}{3} + 1 = 1.0$$\n",
    "\n",
    "So, `cat` will be upweighted relative to `dog`, because it is the less widely used word across documents in the corpus.\n",
    "\n",
    "Our non-normalized but IDF-weighted feature matrix will look like this:\n",
    "\n",
    "```\n",
    "cat  dog\n",
    "1.4  1.0\n",
    "0    2.0\n",
    "```\n",
    "\n",
    "In code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Feature matrix *without* IDF weighting ###\n",
      "Feature names: ['cat' 'dog']\n",
      "[[0.70710678 0.70710678]\n",
      " [0.         1.        ]]\n",
      "\n",
      "### Feature matrix *with* IDF weighting ###\n",
      "Feature names: ['cat' 'dog']\n",
      "[[0.81480247 0.57973867]\n",
      " [0.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    \"cat dog\",\n",
    "    \"dog dog\",\n",
    "]\n",
    "\n",
    "# without IDF weighting (note l2 norm)\n",
    "vectorizer_no_idf = TfidfVectorizer(\n",
    "    use_idf=False\n",
    ")\n",
    "features_no_idf = vectorizer_no_idf.fit_transform(corpus)\n",
    "print(\"### Feature matrix *without* IDF weighting ###\")\n",
    "print(\"Feature names:\", vectorizer_no_idf.get_feature_names_out())\n",
    "print(features_no_idf.toarray())\n",
    "\n",
    "# with IDF weighting\n",
    "vectorizer_with_idf = TfidfVectorizer(\n",
    "    use_idf=True\n",
    "    #norm = None\n",
    "    # ^ would result in the non-normalized scores shown above in markdown.\n",
    ")\n",
    "features_with_idf = vectorizer_with_idf.fit_transform(corpus)\n",
    "print(\"\\n### Feature matrix *with* IDF weighting ###\")\n",
    "print(\"Feature names:\", vectorizer_with_idf.get_feature_names_out())\n",
    "print(features_with_idf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that, in document 1, `cat` has been up-weighted while `dog` has been downweighted. There's no change in document 2 because that document has only a single word type and `TfidfVectorizer`'s `l2` norm enforces total feature weights whose squares sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check our hand calculation against code version\n",
    "import numpy as np\n",
    "vec = np.array([1.405, 1.0])     # hand calculation\n",
    "l2_vec = vec/np.linalg.norm(vec) # calculate l2 normalized version\n",
    "print(\"l2-normed, hand calculated, IDF weighted features for document 1\")\n",
    "print(l2_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# do our two versions match?\n",
    "assert np.allclose(l2_vec, features_with_idf[0,].toarray(), atol=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "We are often interested in the relationships between measured properties of texts, or between a textual property and some continuous variable (year of publication, number of sales, and so on).\n",
    "\n",
    "Maybe the most basic way to measure the relationship between two variables is to use **linear regression**. The idea is to calculate a straight line through your data such that the average distance between the observed data points and the line is as small as possible. \n",
    "\n",
    "(Sketch what this looks like)\n",
    "\n",
    "You can then calculate the **coefficient of determination**, written $r^2$ (\"r squared\"), which measures the fraction of the variation in the dependent (y) variable that is predictable from the independent (x) variable.\n",
    "\n",
    "$r^2$ = 1 - (sum of squared residuals)/(sum of squared values)\n",
    "\n",
    "A *residual* is the amount by which the actual value differs from the predicted value. The \"squared values\" in this equation is the amount by which each actual value differs from the mean of all the actual values. \n",
    "\n",
    "In effect, $r^2$ measures the amount by which the predicted values outperform a simple guess of the mean of the dataset when asked for a prediction of the dependent variable across all observed independent variables. This also explains how it's possible for $r^2$ to be negative: if our model is bad enough, it can perform worse than guessing the mean.\n",
    "\n",
    "An $r^2$ value of 1 indicates perfect correlation between the variables; zero means no correlation. \n",
    "\n",
    "* There's a *lot* more to this. We'll spend some time on it later in the semester.\n",
    "* For now, focus on the fact that regression is a way to calculate a line of best fit through a data set.\n",
    "* Notice that we could also try to find something like a \"line of *worst* fit,\" which we could think of as the dividing line between two regions of feature space. This would be something like the line on which we are least likely to encounter any actual data points. \n",
    "  * Think about what use-value such a dividing line might have ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: character space\n",
    "\n",
    "Let's say we have two competing theories about how narrative space is allocated to characters in novels. These can be summarized as:\n",
    "\n",
    "1. Theory 1: Novels generally contain a fixed amount of character space. If a text allocates more space to characters of one type, it must necessarily allocate less space to characters of other types. This is a constraint imposed by what it means to tell a story.\n",
    "2. Theory 2: There is a (wide?) range of amounts of character space that a novel might use. Some novels spend a lot of time talking about their characters; others, very little. If we see a lot of one type of character, we're likely to see a lot of other character types, too.\n",
    "\n",
    "How could we test these competing models of narrative?\n",
    "\n",
    "First, we need to work out the measurable implications of each model:\n",
    "\n",
    "* We'll use the number of pronouns as a simple proxy for character space.\n",
    "* We'll use pronoun genders as our target for character types.\n",
    "    * Specifically, `she/her/hers` vs. `he/him/his` vs. `they/them/their/theirs`\n",
    "* In both cases, we'll normalize by book length (by dividing the count of each pronoun class by the total number of tokens in the book).\n",
    "* If theory 1 is correct, we'll expect to see fewer `she` pronouns in books that use more `he` pronouns. Ditto `he/they` and `she/they`.\n",
    "* If theory 2 is correct, we'll expect to see the opposite: more pronouns of all types when any one of the types is higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# pronoun sets\n",
    "f_words = ['she', 'her', 'hers']\n",
    "m_words = ['he', 'him', 'his']\n",
    "t_words = ['they', 'them', 'their', 'theirs']\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    use_idf=False,\n",
    "    norm=None,\n",
    "    lowercase=True,\n",
    "    strip_accents='unicode',\n",
    "    input='filename',\n",
    "    encoding='utf-8',\n",
    "    #vocabulary= f_words + m_words + t_words\n",
    ")\n",
    "\n",
    "data_dir = os.path.join('..', 'data', 'texts')\n",
    "files = os.listdir(data_dir)\n",
    "\n",
    "# vectorize and length-normalize\n",
    "data = vectorizer.fit_transform([os.path.join(data_dir, filename) for filename in files])\n",
    "data = pd.DataFrame.from_records(data.toarray(), columns=vectorizer.get_feature_names_out(), index=files)\n",
    "data = data.div(data.sum(axis='columns'), axis='rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_type(data, words, norm=True):\n",
    "    '''A function to gather and sum words of a single type. Z-scores output by default.'''\n",
    "    output = data.loc[:,words].sum(axis='columns')\n",
    "    if norm:\n",
    "        output = (output - output.mean())/output.std()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot m vs f pronoun classes\n",
    "sns.regplot(x=process_type(data, f_words), y=process_type(data, m_words))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fit data and score model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(\n",
    "    X=pd.DataFrame(process_type(data, f_words)), \n",
    "    y=process_type(data, m_words))\n",
    "\n",
    "print(f\"Coefficient: {regressor.coef_.item():>7.4f}\")\n",
    "print(f\"Intercept:   {regressor.intercept_.item():>7.4f}\")\n",
    "print(f\"R^2:         {regressor.score(X=pd.DataFrame(process_type(data, f_words)), y=process_type(data, m_words)):>7.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x=process_type(data, f_words), y=process_type(data, m_words))\n",
    "plt.plot(process_type(data, f_words), regressor.predict(X=pd.DataFrame(process_type(data, f_words))), c='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pairs = {\n",
    "    'He-she': (m_words, f_words),\n",
    "    'He-they': (m_words, t_words),\n",
    "    'She-they': (f_words, t_words)\n",
    "}\n",
    "\n",
    "def compare_types(data, dep_words, indep_words, name=''):\n",
    "    regressor = LinearRegression()\n",
    "    regressor.fit(X=pd.DataFrame(process_type(data, indep_words)), y=process_type(data, dep_words))\n",
    "\n",
    "    print(name)\n",
    "    print(f\"  Coefficient: {regressor.coef_.item():>7.4f}\")\n",
    "    print(f\"  Intercept:   {regressor.intercept_.item():>7.4f}\")\n",
    "    print(f\"  R^2:         {regressor.score(X=pd.DataFrame(process_type(data, indep_words)), y=process_type(data, dep_words)):>7.4f}\")\n",
    "    print()\n",
    "    return(regressor)\n",
    "\n",
    "for key in pairs:\n",
    "    compare_types(data, *pairs[key], name=key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm ... these effects are really weak. Is something broken? Let's try terms that we expect to be strongly correlated ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_pairs = {\n",
    "    'Whale-fish': (['whale'], ['fish']),\n",
    "    'God-heaven': (['god'], ['heaven']),\n",
    "    'Mother-son': (['mother'], ['son']),\n",
    "    '1st p-3rd p': (['me', 'my', 'mine'], ['he', 'his', 'she', 'her']),\n",
    "}\n",
    "for key in new_pairs:\n",
    "    compare_types(data, *new_pairs[key], name=key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so there *are* correlations in the data, just not unambiguous evidence to help us decide between out competing theories.\n",
    "\n",
    "Finally, let's get the most strongly correlated terms in the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get 1000 most common words in the corpus\n",
    "top_terms = data.sum(axis='rows').sort_values().tail(1000).index\n",
    "\n",
    "# calculate correlations and remove self-correlations\n",
    "correlations = data.loc[:,top_terms].corr()\n",
    "np.fill_diagonal(correlations.values, np.nan)\n",
    "\n",
    "# examine most correlated and anti-correlated terms\n",
    "size = len(correlations)\n",
    "print(\"Most positively correlated:\")\n",
    "for j, i in enumerate(np.argsort(correlations, axis=None)[-1040:-1000][::-1]):\n",
    "    if j%2==0:\n",
    "        rowidx = correlations.index[i//size]\n",
    "        colidx = correlations.columns[i%size]\n",
    "        value = correlations.loc[rowidx, colidx]\n",
    "        print(f\"{rowidx:<10} {colidx:<10} {value:7.4f}\")\n",
    "        \n",
    "print(\"\\nMost negatively correlated:\")\n",
    "for j, i in enumerate(np.argsort(correlations, axis=None)[:40]):\n",
    "    if j%2==0:\n",
    "        rowidx = correlations.index[i//size]\n",
    "        colidx = correlations.columns[i%size]\n",
    "        value = correlations.loc[rowidx, colidx]\n",
    "        print(f\"{rowidx:<10} {colidx:<10} {value:7.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check out the volume-wise values for a single term\n",
    "data.loc[:,'jimmie'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# strong positive correlation\n",
    "sns.regplot(data, x='didn', y='wouldn');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# strong negative correlation\n",
    "sns.regplot(data, x='of', y='go');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# how does the TfidfVectorizer handle contractions?\n",
    "vec = TfidfVectorizer(input='content')\n",
    "vec.fit_transform([\"wouldn't\", \"can't\"])\n",
    "vec.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
