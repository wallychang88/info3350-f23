{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO 3350/6350\n",
    "\n",
    "## Lecture 01: Tokenization and word counts\n",
    "\n",
    "## To do\n",
    "\n",
    "* Readings for next week (see [syllabus schedule](https://github.com/wilkens-teaching/info3350-f23/blob/main/schedule.md))\n",
    "  * Mon: Reagan et al.+\n",
    "  * Weds: Ramsay (Canvas), Healy, Rambsy\n",
    "  * Come to lecture prepared: have a question, thought, or connection between the readings\n",
    "  * Responses by Tuesday at 4pm for students with NetIDs `a*-g*`; see [instructions on Canvas](https://canvas.cornell.edu/courses/57246/discussion_topics)\n",
    "      * Respond to **Wednesday** reading, not Monday\n",
    "* Go to section this week!\n",
    "    * If you hope to switch sections, go to desired section and ask if anyone will swap. You might also try posting to Ed with your request.\n",
    "        * If yes, email [courses@cis.cornell.edu](mailto:courses@cis.cornell.edu) with both NetIDs and they will make the swap\n",
    "    * If not yet enrolled from waitlist, attend any section\n",
    "    \n",
    "## The question: How do we turn books into data?\n",
    "\n",
    "What specific things might we do to make books into computable objects? 30 seconds with the person next to you ...\n",
    "\n",
    "## Definition\n",
    "\n",
    "What is a token?\n",
    "\n",
    "* The **smallest individually meaningful unit of a document.** Roughly, a word.\n",
    "* But ... as soon as you see \"meaningful,\" you know it's going to be a matter of interpretation.\n",
    "  * *Every single thing you do in text analysis is an interpretive intervention!*\n",
    "* Not all tokens are (single) words. For example:\n",
    "  * **Contractions**. `\"I'm\"` or `\"can't\"`. One token or two?\n",
    "  * **Phrases.** `\"San Francisco\"` or `\"Cornell University\"`. Two tokens or one?\n",
    "    * These are exampled of \"named entities.\" We'll revisit them later in the semester.\n",
    "  * **Punctuation.** Count it at all? Is `\"this\"` the same token as `\"this!\"`? Is `\".\"` or `\";\"` a token on its own?\n",
    "  * **Domain-specific terms.** `\"@user\"`, `\"COVID-19\"`, etc.\n",
    "\n",
    "## Why tokenize?\n",
    "\n",
    "Words suggest meaning. This is the wager and the starting point of many text analysis methods.\n",
    "  * If we can identify words, we can count them.\n",
    "  * Words are small enough to recur, so not all counts are `1` (which isn't very informative)\n",
    "    * Hence we can compare word counts across contexts\n",
    "    * Compare to sentences (or paragraphs, or full documents), which are often globally unique\n",
    "  * If we we can count words, we can quantify (aspects of) a text that contains those words.\n",
    "  * **If we can quantify a text, we can compute with it.**\n",
    "  * **This is the most common way that text becomes data!**\n",
    "\n",
    "Note that quantifying a text isn't the same thing as being *correct* about what that text means, nor is meaning solely a function of word counts(!).\n",
    "\n",
    "Tokenization is part of the more-or-less standard text-processing workflow. Other parts of that workflow might include:\n",
    "  * Case regularization/folding\n",
    "  * Punctuation removal\n",
    "  * Lemmatization or stemming\n",
    "  * Sentence segmentation\n",
    "  * and more ...\n",
    "  \n",
    "## State of the art\n",
    "\n",
    "A decade ago, using raw tokens for NLP tasks was the best we could do. Today, we generally use static or contextual word *embeddings* in place of tokens. We'll talk about this at length in the second half of the course, but the underlying idea is the same. Words and embeddings are proxies for meaning (which is what we ultimately care about, but is never directly accessible to us). Embeddings are just a way to capture more of the specific meaning of a word as it is used in a given language (static) or linguistic context (contextual).\n",
    "\n",
    "## Tokenization can be domain-specific\n",
    "\n",
    "Note that today's reading assumed some special interests:\n",
    "\n",
    "* Twitter(like) texts\n",
    "* Sentiment as target phenomenon\n",
    "\n",
    "So it worked hard to capture Twitter handles, hashtags, smilies, URLs, etc.\n",
    "\n",
    "The \"right\" way to tokenize depends on your project, on what is meaningful *in context*.\n",
    "If you have different data or different phenomena to investigate, you might tokenize differently.\n",
    "\n",
    "## Approach 1: Split on whitespace\n",
    "\n",
    "A simple, naÃ¯ve approach, workable for quick-and-dirty work with many Western languages.\n",
    "\n",
    "Consider the sentence:\n",
    "\n",
    "> Cornell is a private, Ivy League university and the land-grant university for New York state.\n",
    "\n",
    "How many tokens does this sentence contain? (count them for yourself)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cornell', 'is', 'a', 'private,', 'Ivy', 'League', 'university', 'and', 'the', 'land-grant', 'university', 'for', 'New', 'York', 'state.']\n",
      "Number of tokens: 15\n"
     ]
    }
   ],
   "source": [
    "cornell = 'Cornell is a private, Ivy League university and the land-grant university for New York state.'\n",
    "tokens = cornell.split()\n",
    "print(tokens)\n",
    "print(\"Number of tokens:\", len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice: `private,` `land-grant` `state.` These aren't wrong *per se*, but ...\n",
    "\n",
    "Maybe we could do better if we just took non-space, non-puctuation strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cornell', 'is', 'a', 'private', 'Ivy', 'League', 'university', 'and', 'the', 'land', 'grant', 'university', 'for', 'New', 'York', 'state']\n",
      "Number of tokens: 16\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "word_pattern = re.compile(\"[\\w]+\")\n",
    "tokens_re = word_pattern.findall(cornell)\n",
    "print(tokens_re)\n",
    "print(\"Number of tokens:\", len(tokens_re))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular expressions\n",
    "\n",
    "A totally inadequate mini-introduction to an important but annoyingly complex technology.\n",
    "\n",
    "* What is a regular expression (regex)?\n",
    "  * A sequence of characters that define a search pattern.\n",
    "  * That is, it's a text search or matching language.\n",
    "  * Notoriously unreadable and difficult to parse by eye.\n",
    "  \n",
    "Consider the line above:\n",
    "\n",
    "```\n",
    "word_pattern = re.compile(\"[\\w]+\")\n",
    "```\n",
    "\n",
    "The search pattern here is any sequence of one or more (`+`) uniterrupted \"word\" characters (`\\w` = upper- and lowercase letters, plus digits) that occur anywhere in a string. Regexes are usually \"greedy,\" so will continue matching character by character until their condition is not met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t']\n",
      "['the']\n",
      "['these']\n",
      "['these', 'uns']\n",
      "['these', 'ones']\n"
     ]
    }
   ],
   "source": [
    "for word in ['t', 'the', 'these', \"these'uns\", \"these ones\"]:\n",
    "    print(word_pattern.findall(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`re` is Python's regular expression library. `compile` prepares the regular expression for use with text inputs.\n",
    "\n",
    "A few other useful bits of regex syntax:\n",
    "\n",
    "* `.` (period) = any character\n",
    "* `\\s` = whitespace character (space, tab, newline, etc.)\n",
    "* `\\d` = digit\n",
    "* `[abc]` = any character in the set {a, b, c}.\n",
    "* `[^abc]` = negation, any character *except* a, b, or c.\n",
    "* `A*` = zero or more occurrences of the character A; `+` = one or more, `?` = zero or one.\n",
    "* `\\A`, `\\Z`, `^`, and `$` = match only at start or end of a string or line, respectively.\n",
    "* `\\` (backslash) = escape the next character; `\\.` = period, not wildcard.\n",
    "\n",
    "There's a lot more to this. Take a look at the code linked from today's reading, and/or consult a [regex cheat sheet](https://learnbyexample.github.io/cheatsheet/python/python-regex-cheatsheet/).\n",
    "\n",
    "Why use regular expressions?\n",
    "  * A powerful way to find/match/extract substrings from strings and texts.\n",
    "  * Can use regexes to build robust custom tokenizers (as in the reading for today)\n",
    "\n",
    "### NLTK\n",
    "\n",
    "The Natural Language Tool Kit (NLTK) is a full-featured Python NLP library. It includes a bunch of tokenizers, nearly all of them extensible, that will probably perform better than whatever you can hack together for your project.\n",
    "\n",
    "Let's try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cornell', 'is', 'a', 'private', ',', 'Ivy', 'League', 'university', 'and', 'the', 'land-grant', 'university', 'for', 'New', 'York', 'state', '.']\n",
      "Number of tokens: 17\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "tokens_nltk = word_tokenize(cornell)\n",
    "print(tokens_nltk)\n",
    "print('Number of tokens:', len(tokens_nltk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ca', \"n't\", ',', 'I', \"'m\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"can't, I'm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that NLTK treats word-terminal punctuation as a token and is smart about contractions.\n",
    "\n",
    "## Non-English/Non-Western text\n",
    "\n",
    "Whitespace can be a very bad approach if Western typographic conventions don't apply!\n",
    "\n",
    "If you don't know the language:\n",
    "\n",
    "* Ask if you should be doing the work\n",
    "* Lean on libraries\n",
    "\n",
    "### Example from the *New York Times*\n",
    "\n",
    "In a [recent *Times* article](https://www.nytimes.com/2020/09/03/sports/soccer/premier-league-china-contract-television.html) on football broadcasting rights, we find this sentence:\n",
    "\n",
    "**Chinese**\n",
    "\n",
    "> å åæ°åå ç¶çæ¯å±æºå¯¹è¶³çåå¶ä»ä½è²èµäºçæç»­å½±åï¼æ©å·²é¢ä¸´è¶æ¥è¶å¤äºæçè±æ ¼å°è¶çº§è¶³çèèµå¨åå®£å¸ï¼å ä¸ºæ æ³è§£å³ä¸ä¸­å½åä½ä¼ä¼´ççº çº·ï¼å·²ç»æ­¢äºå¶æèµé±çæµ·å¤è½¬æ­ååã\n",
    "\n",
    "**English translation**\n",
    "\n",
    "> The English Premier League, already facing mounting losses because of the continued impact of the coronavirus crisis on soccer and other sporting events, announced on Thursday that it had canceled its most lucrative overseas broadcast contract after it was unable to resolve a dispute with its Chinese partner.\n",
    "\n",
    "Our previous tokenization strategy doesn't work well in this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['å åæ°åå ç¶çæ¯å±æºå¯¹è¶³çåå¶ä»ä½è²èµäºçæç»­å½±åï¼æ©å·²é¢ä¸´è¶æ¥è¶å¤äºæçè±æ ¼å°è¶çº§è¶³çèèµå¨åå®£å¸ï¼å ä¸ºæ æ³è§£å³ä¸ä¸­å½åä½ä¼ä¼´ççº çº·ï¼å·²ç»æ­¢äºå¶æèµé±çæµ·å¤è½¬æ­ååã']\n",
      "Number of Chinese tokens: 1\n",
      "Number of English tokens: 48\n"
     ]
    }
   ],
   "source": [
    "# Strings\n",
    "zh = 'å åæ°åå ç¶çæ¯å±æºå¯¹è¶³çåå¶ä»ä½è²èµäºçæç»­å½±åï¼æ©å·²é¢ä¸´è¶æ¥è¶å¤äºæçè±æ ¼å°è¶çº§è¶³çèèµå¨åå®£å¸ï¼å ä¸ºæ æ³è§£å³ä¸ä¸­å½åä½ä¼ä¼´ççº çº·ï¼å·²ç»æ­¢äºå¶æèµé±çæµ·å¤è½¬æ­ååã'\n",
    "en = 'The English Premier League, already facing mounting losses because of the continued impact of the coronavirus crisis on soccer and other sporting events, announced on Thursday that it had canceled its most lucrative overseas broadcast contract after it was unable to resolve a dispute with its Chinese partner.'\n",
    "\n",
    "# Naive approach to tokenization\n",
    "zh_tokens_bad = zh.split()\n",
    "print(zh_tokens_bad)\n",
    "print('Number of Chinese tokens:', len(zh_tokens_bad))\n",
    "\n",
    "# English version\n",
    "en_tokens = en.split()\n",
    "print('Number of English tokens:', len(en_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `jieba` tokenizer\n",
    "\n",
    "See the [`jieba` project GitHub page](https://github.com/fxsjy/jieba) for documentation (in Chinese and in English). `jieba` is one of the packages we installed in our virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/xd/m092nj891q71xlv9zcn1sd8r0000gn/T/jieba.cache\n",
      "Loading model cost 0.292 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['å å', 'æ°å', 'å ç¶çæ¯', 'å±æº', 'å¯¹', 'è¶³ç', 'å', 'å¶ä»', 'ä½è²èµäº', 'ç', 'æç»­', 'å½±å', 'ï¼', 'æ©å·²', 'é¢ä¸´', 'è¶æ¥è¶', 'å¤', 'äºæ', 'ç', 'è±æ ¼å°', 'è¶çº§', 'è¶³çèèµ', 'å¨å', 'å®£å¸', 'ï¼', 'å ä¸º', 'æ æ³', 'è§£å³', 'ä¸', 'ä¸­å½', 'åä½ä¼ä¼´', 'ç', 'çº çº·', 'ï¼', 'å·²', 'ç»æ­¢', 'äº', 'å¶', 'æ', 'èµé±', 'ç', 'æµ·å¤', 'è½¬æ­', 'åå', 'ã']\n",
      "Number of Chinese tokens: 45\n"
     ]
    }
   ],
   "source": [
    "# A better approach to tokenizing Chinese-language text\n",
    "import jieba\n",
    "zh_tokens_better = [token for token in jieba.cut(zh)]\n",
    "print(zh_tokens_better)\n",
    "print(\"Number of Chinese tokens:\", len(zh_tokens_better))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting words\n",
    "\n",
    "We often want to count the number of occurrences of each *unique* type of token in a text.\n",
    "\n",
    "Note that '**type**' is a quasi-technical word that means \"unique token form.\" The sentence:\n",
    "\n",
    "> The cat is a cat.\n",
    "\n",
    "... contains five tokens, but only four types. We find the same term (and concept) in the measure of **type-token ratio** (TTR), which we can use to measure the lexical diversity of a text. Note that \"lexical diversity\" does not equal \"sophistication\" or \"value.\" Gertrude Stein's poetry has low lexical diversity. Hemingway is surprisingly high. Pulp fiction often has (much) higher TTR than does \"literary\" fiction.\n",
    "\n",
    "Anyway ... if we count tokens, throwing away word order, we have transformed our text(s) into a so-called \"**bag of words**.\"\n",
    "\n",
    "### Bags of words\n",
    "\n",
    "A bag of words is a **representation** of a text in the same way that a photograph or a story might be a representation of a person. It's a way of looking at the text, useful for some purposes, terribly inadequate for others. \n",
    "\n",
    "A bag of words is neither a good nor a bad representation *in the abstract*, because there is no such thing as an abstractly (or universally) good or bad representation. Goodness and badness only apply to the suitability of a representation for a particular purpose in a specific context.\n",
    "\n",
    "### Let's count some types and tokens\n",
    "\n",
    "Approach: Iterate over a list of tokens, counting the number of times we see each unique type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Cornell': 1,\n",
       "         'is': 1,\n",
       "         'a': 1,\n",
       "         'private': 1,\n",
       "         ',': 1,\n",
       "         'Ivy': 1,\n",
       "         'League': 1,\n",
       "         'university': 2,\n",
       "         'and': 1,\n",
       "         'the': 1,\n",
       "         'land-grant': 1,\n",
       "         'for': 1,\n",
       "         'New': 1,\n",
       "         'York': 1,\n",
       "         'state': 1,\n",
       "         '.': 1})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "cornell_counter = Counter() # easier than using a dict (why?)\n",
    "for token in tokens_nltk:\n",
    "    cornell_counter[token] += 1\n",
    "cornell_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cornell_counter['university']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('university', 2), ('Cornell', 1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cornell_counter.most_common(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cornell_counter['coffee']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Way more words!\n",
    "\n",
    "Let's count the words in *Moby-Dick* (Herman Melville, 1851), sometimes called \"the great American novel.\" It's long: 500 pages or more, depending on the edition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# path to text file (available on course GitHub)\n",
    "import os\n",
    "fn = os.path.join('..', 'data', 'texts', 'A-Melville-Moby_Dick-1851-M.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/texts/A-Melville-Moby_Dick-1851-M.txt'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the file path constructed above\n",
    "fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 68.9 ms, sys: 6.14 ms, total: 75.1 ms\n",
      "Wall time: 84.7 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('the', 13603),\n",
       " ('of', 6475),\n",
       " ('and', 5881),\n",
       " ('a', 4473),\n",
       " ('to', 4439),\n",
       " ('in', 3825),\n",
       " ('that', 2680),\n",
       " ('his', 2415),\n",
       " ('I', 1724),\n",
       " ('with', 1645)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# naive but fast\n",
    "moby_fast = Counter()\n",
    "with open(fn, 'r') as f:\n",
    "    for line in f: # read one line at a time for memory efficiency\n",
    "        mtokens = line.strip().split() # strip newlines, split on space\n",
    "        for token in mtokens:\n",
    "            moby_fast[token] += 1\n",
    "moby_fast.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 608 ms, sys: 12.1 ms, total: 620 ms\n",
      "Wall time: 624 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(',', 19204),\n",
       " ('the', 13715),\n",
       " ('.', 7432),\n",
       " ('of', 6513),\n",
       " ('and', 6010),\n",
       " ('a', 4546),\n",
       " ('to', 4515),\n",
       " (';', 4173),\n",
       " ('in', 3909),\n",
       " ('that', 2981)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# better but slower\n",
    "moby_nltk = Counter()\n",
    "with open (fn, 'r') as f:\n",
    "    for line in f:\n",
    "        mtokens = word_tokenize(line)\n",
    "        for token in mtokens:\n",
    "            moby_nltk[token] += 1\n",
    "moby_nltk.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in Moby-Dick (per split): 212014\n",
      "Number of words in Moby-Dick (per NLTK):  255370\n"
     ]
    }
   ],
   "source": [
    "# Total wordcount\n",
    "print(\"Number of words in Moby-Dick (per split):\", sum(moby_fast.values()))\n",
    "print(\"Number of words in Moby-Dick (per NLTK): \", sum(moby_nltk.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords and other processing steps\n",
    "\n",
    "Notice that the most frequently occurring words in *Moby-Dick* don't carry much meaning on their own.\n",
    "\n",
    "These high-frequency tokens are sometimes called *stopwords*. Stopwords are words that one wants to remove from one's token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base stoplist: ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "\n",
      "Our stoplist: ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n"
     ]
    }
   ],
   "source": [
    "# Work with sample stopwords\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stops = stopwords.words('english') # NLTK's short list of English stopwords\n",
    "print(\"Base stoplist:\", stops)\n",
    "\n",
    "for punct in string.punctuation:\n",
    "    stops.append(punct) # Add punctuation marks to stoplist\n",
    "print(\"\\nOur stoplist:\", stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 2121),\n",
       " (\"'s\", 1731),\n",
       " ('--', 1714),\n",
       " (\"''\", 1565),\n",
       " ('``', 1529),\n",
       " ('one', 881),\n",
       " ('whale', 789),\n",
       " ('But', 703),\n",
       " ('The', 609),\n",
       " ('like', 558)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for stop in stops:\n",
    "    del moby_nltk[stop]\n",
    "moby_nltk.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in Moby-Dick (per NLTK, minus stopwords):  123712\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of words in Moby-Dick (per NLTK, minus stopwords): \", sum(moby_nltk.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB.** Consider the difference in wordcount after removing stopwords ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More processing\n",
    "\n",
    "Consider how and why you might do each of the following:\n",
    "\n",
    "* Case regularization (all lower case?)\n",
    "  * `lower()` string method\n",
    "* Punctuation removal\n",
    "  * At what point(s) in the process?\n",
    "  * (Dis)advantages of each?\n",
    "* Lemmatization\n",
    "  * `from nltk.stem import WordNetLemmatizer`\n",
    "  * Stemming is faster but less accurate\n",
    "  * Note that lemmatization benefits from knowledge of token part of speech. Part-of-speech taggers benefit from case and punctuation information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am -> am\n",
      "am (verb) -> be\n",
      "wolves -> wolf\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print('am ->', lemmatizer.lemmatize('am'))\n",
    "print('am (verb) ->', lemmatizer.lemmatize('am', pos='v'))\n",
    "print('wolves ->', lemmatizer.lemmatize('wolves'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A word on seeking help in this class ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Ed policies\n",
    "\n",
    "* Ed is the first-best place to ask questions!\n",
    "  * If you have a question, other people probably do, too\n",
    "* You may post anonymously (to other students), but staff can always see your name\n",
    "* Modest extra credit for high-quality participation on Ed, especially answering (correctly) other student's questions\n",
    "* Staff wait 24 hours to respond (by policy) unless the matter is urgent\n",
    "  * Homework due tomorrow is not urgent\n",
    "\n",
    "### How to ask good questions\n",
    "\n",
    "* More info, carefully curated, is better than less info\n",
    "  * What homework? What code? **What are you trying to do?** What does your data look like? What output did you get? What did you expect to happen? What error message? Where does the data live? What did you try? **What does the documentation say?** ...\n",
    "  * Craft a minimal example demonstrating the problem (this often helps you solve it on your own)\n",
    "* Remember that TAs (and even professors) are human\n",
    "  * They want to help. Respect, gratitude, and patience go a long way.\n",
    "  * Evidence that you've made an earnest attempt to solve the problem on your own also goes a long way.\n",
    "  \n",
    "### Whom and where to ask\n",
    "\n",
    "**When in doubt -> Ed!** Always open, async, monitored by staff, extra credit.\n",
    "\n",
    "* It's 3am and my code won't work -> Maybe sleep on it?\n",
    "* It's 9am and my code won't work -> Ed, office hours\n",
    "* It's the next day and no one can figure it out -> Office hours (grad)\n",
    "* I'm looking for project/HW partners -> Ed, lecture, [Learning Strategies Center](https://lsc.cornell.edu/)\n",
    "* I'm thinking about grad school -> Office hours (grad, Prof. Wilkens)\n",
    "* I have questions about the major, future classes, or careers -> Prof. Wilkens OH\n",
    "* I'm really struggling (in the class, in general) -> Any staff, any time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
